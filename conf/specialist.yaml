# conf/specialist.yaml
# PHASE 2: Unified Specialist (APEX-MoE) configuration.

defaults:
  - /dataset: default
  - _self_

# --- Experiment Metadata ---
run_name: "apex_v1_${now:%Y-%m-%d_%H-%M-%S}"
seed: 42
compile_model: true

# --- Directory Paths ---
output_dir: "outputs/phase2"
checkpoint_dir: "outputs/phase2/checkpoints"
log_dir: "outputs/phase2/logs"
backup_dir: null
resume_from: null

# --- Model Architecture ---
# MUST matches Phase 1 for weight loading.
model:
  input_dim: 28
  static_dim: 6
  history_len: 24
  pred_len: 6
  d_model: 768
  n_heads: 12
  n_layers: 8
  encoder_layers: 4
  ffn_dim_ratio: 4
  dropout: 0.1
  use_rope: true
  use_swiglu: true
  use_flash_attn: true
  gradient_checkpointing: true
  timesteps: 100
  beta_schedule: "cosine"
  prediction_type: "epsilon"
  use_ddim_sampling: true
  use_auxiliary_head: true
  num_phases: 6 # SOTA-Clinical: 6 experts with Top-2 routing

# --- Training / Specialist Hyperparameters ---
train:
  phase: "phase2"
  # Mandatory for Phase 2: Update this path after Phase 1!
  pretrained_path: "outputs/phase1/gen_v1_.../checkpoints/latest_checkpoint.pt"
  
  epochs: 50
  batch_size: 128
  num_workers: 16
  lr: 2.0e-4
  min_lr: 1e-6
  warmup_steps: 500
  weight_decay: 1e-4
  grad_clip: 1.0
  precision: "16-mixed"
  accumulate_grad_batches: 1
  log_every_n_steps: 10
  
  log_every_n_steps: 10
  
  # SOTA 2025 Balancing ("legacy_surgical" or "sota_2025")
  balancing_mode: "sota_2025" 
  uw_lr: 0.025                # Learning rate for uncertainty weights
  crash_weight: 5.0   # Up-weight Sepsis gradients
  lambda_reg: 0.05    # Cross-expert tether (prevents mode drift)
  
  # AWR Optimization v2.0 (Globally Optimal via Exhaustive Audit)
  awr_beta: 0.400             # Optimized for Specialist Fine-Tuning
  awr_max_weight: 20.0        # Robust Clipping
  
  # AWR Calibration Speed/Quality (v10.1 Upgrade)
  awr_calibration_mode: "sample"
  awr_max_samples: 5000

  
  aux_loss_scale: 0.182 # Optimized v2.0 (Router Supervision)
  ema_decay: 0.999    # Faster EMA update for adaptation
  
  # Optimization Groups
  backbone_lr_ratio: 1.0
  backbone_modules: []

# --- Logging ---
logging:
  use_wandb: true
  wandb_project: "icu-diffusion-phase2"
  wandb_mode: "offline"
  run_name: ${run_name}
